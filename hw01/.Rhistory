ratings$r.year/2000
(ratings$r.year/2000) / (max(ratings$r.year)/2000))
((ratings$r.year/2000) / (max(ratings$r.year)/2000))
((ratings$r.year/2000) / (max(ratings$r.year)/1000))
1/ log(ratings$r.year)
1/ log(ratings$r.year + exp(1))
(ratings$r.year^10 / max(ratings$r.year)^10)
summary((ratings$r.year^10 / max(ratings$r.year)^10))
summary((ratings$r.year^100 / max(ratings$r.year)^100))
summary((ratings$r.year^15 / max(ratings$r.year)^15))
summary((ratings$r.year^25 / max(ratings$r.year)^25))
summary((ratings$r.year^40 / max(ratings$r.year)^40))
summary((ratings$r.year^50 / max(ratings$r.year)^50))
ratings$scale <- (ratings$r.year^50 / max(ratings$r.year)^50) * (ratings$m.year^50 / max(ratings$m.year)^50)
summary(ratings$scale)
ratings$scale <- (ratings$r.year^25 / max(ratings$r.year)^25) * (ratings$m.year^25 / max(ratings$m.year)^25)
summary(ratings$scale)
knitr::kable(head(ratings))
ratings$rating.scaled <- ratings$rating * ratings$scale
knitr::kable(head(ratings))
summary(ratings$rating.scaled)
summary(ratings$rating)
ratings_hori <- acast(ratings, userId ~ movieId, value.var="rating")
ratings_rm <- as(ratings_hori, "realRatingMatrix")
ratings_rm_r <- ratings_rm[, colCounts(ratings_rm) > 20]
movies.recom <- Recommender(ratings_rm_r[1:400], method = "UBCF")
movies.predict <- predict(movies.recom, ratings_rm_r[c(500, 525, 600)],n=5)
knitr::kable(as.data.frame(as(movies.predict, "list")))
movies.predict, "list")
as(movies.predict, "list")
movies.predict <- as.data.frame(as(movies.predict, "list"))
knitr::kable(movies.predict)
library(knitr)
kable(movies.predict)
movies.recom <- Recommender(ratings_rm_r[1:400], method = "UBCF")
movies.predict <- predict(movies.recom, ratings_rm_r[c(500, 525, 600)],n=3)
movies.predict <- as.data.frame(as(movies.predict, "list"))
kable(movies.predict)
ratings_hori <- acast(ratings, userId ~ movieId, value.var="rating.scaled")
ratings_rm <- as(ratings_hori, "realRatingMatrix")
ratings_rm_r <- ratings_rm[, colCounts(ratings_rm) > 20]
movies.recom <- Recommender(ratings_rm_r[1:400], method = "UBCF")
movies.predict <- predict(movies.recom, ratings_rm_r[c(500, 525, 600)],n=3)
movies.predict <- as.data.frame(as(movies.predict, "list"))
kable(movies.predict)
movies.o <- movies[,3]
genres.split <- unlist(strsplit(movies.o, split = "|", fixed = TRUE))
genres.count <- data.frame(table(genres.split))
knitr::kable(head(genres.count[order(-genres.count$Freq),],10))
if(!require(installr)) {
install.packages("installr"); require(installr)} #load / install+load installr
updateR()
Sys.setenv(SPARK_HOME = "C:/spark")
.libPaths(c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"), .libPaths()))
sc <- sparkR.init(master = "local")
sqlContext <- sparkRSQL.init(sc)
library(SparkR)
library(reshape2)
library(dplyr)
library(recommenderlab)
library(ggplot2)
library(stringr)
library(knitr)
library(jsonlite)
Sys.setenv(SPARK_HOME = "C:/spark")
.libPaths(c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"), .libPaths()))
sc <- sparkR.init(master = "local")
sqlContext <- sparkRSQL.init(sc)
movies <- read.csv("https://raw.githubusercontent.com/maxwagner/643/master/ml-latest-small/movies.csv", stringsAsFactors = FALSE)
ratings <- read.csv("https://raw.githubusercontent.com/maxwagner/643/master/ml-latest-small/ratings.csv", stringsAsFactors = FALSE)
ratings.json <- toJSON(ratings)
ratings.json
qq
head(ratings.json)
library(rvest)
library(rvest)
library(psych)
# stripping html tags
stripHtml <- function(htmlString) {
return(gsub("<.*?>", "", htmlString))
}
# strip spaces and line breaks
stripOther <- function(htmlString) {
htmlString <- gsub("\n", "", htmlString)
htmlString <- gsub(" ", "", htmlString)
return(htmlString)
}
# maplist
html <- read_html("http://195.93.242.155/~quake2/quake2/jump/_html/maps.html")
maplist <- html_nodes(html, "div")
maplist <- stripHtml(maplist)
maplist <- stripOther(maplist)
maplist <- data.frame(matrix(maplist, ncol = 5, byrow = TRUE))
maplist <- maplist[-1,]
maplist <- data.frame(maplist$X2)
# map loop
length <- length(maplist[[1]])
prefix <- "http://195.93.242.155/~quake2/quake2/jump/_html/"
suffix <- ".html"
usermaps <- data.frame(NULL)
i <- 1
while (i < length + 1) {
if (grepl("#", maplist[i,])) {}
else {
html <- read_html(paste(prefix, maplist[i,], suffix, sep = ""))
mapfile <- html_nodes(html, "div")
mapfile <- stripHtml(mapfile)
mapfile <- stripOther(mapfile)
mapfile <- data.frame(matrix(mapfile, ncol = 5, byrow = TRUE))
mapfile <- mapfile[-1, -c(4)]
mapfile <- cbind(mapfile, maplist[i,])
usermaps <- rbind(usermaps, mapfile)
}
print(paste(i, "/", length, sep = ""))
i <- i + 1
}
colnames(usermaps) <- c("pos", "name", "date", "time", "map")
write.csv(usermaps, file = "maptimes.csv")
ace <- subset(usermaps, name == "Ace")
write.csv(ace, file = "ace_may16.csv")
first <- subset(usermaps, pos == 1)
write.csv(first, file = "first.csv")
# read in a maptimes condump
filename <- 'C:/quake2/jump/condumps/scores.txt'
# read in condump
scores <- scan(filename, what = 'raw', sep = '\n')
scores2 <- c()
i <- 1
for(x in scores) {
if(grepl('^\\d', x)) {
scores2[i] <- x
i <- i + 1
}
}
# write to a new text file for cleaner reading
write.table(scores2, 'cleaned.txt', row.names = FALSE, col.names = FALSE, quote = FALSE)
# read in new text file
scores <- read.table('cleaned.txt', header = FALSE)
scores <- scores[1:2]
#assign point values
points <- c(15:1)
scores$points <- points[scores$V1]
# aggregate the points for each person
scorescount <- aggregate(scores$points, by = list(scores$V2), FUN = length)
scores <- aggregate(scores$points, by = list(scores$V2), FUN = sum)
scores <- merge(x = scores, y = scorescount, by = "Group.1")
colnames(scores) = c('name','points','maps')
# calc avg finish
scores$avg.finish <- 16 - round(scores$points / scores$maps, 1)
# order by score
scores <- scores[order(scores$points, decreasing = TRUE),]
rownames(scores) <- NULL
scores
# read in a maptimes condump
filename <- 'C:/quake2/jump/condumps/scores.txt'
# read in condump
scores <- scan(filename, what = 'raw', sep = '\n')
scores2 <- c()
i <- 1
for(x in scores) {
if(grepl('^\\d', x)) {
scores2[i] <- x
i <- i + 1
}
}
# write to a new text file for cleaner reading
write.table(scores2, 'cleaned.txt', row.names = FALSE, col.names = FALSE, quote = FALSE)
# read in new text file
scores <- read.table('cleaned.txt', header = FALSE)
scores <- scores[1:2]
#assign point values
points <- c(15:1)
scores$points <- points[scores$V1]
# aggregate the points for each person
scorescount <- aggregate(scores$points, by = list(scores$V2), FUN = length)
scores <- aggregate(scores$points, by = list(scores$V2), FUN = sum)
scores <- merge(x = scores, y = scorescount, by = "Group.1")
colnames(scores) = c('name','points','maps')
# calc avg finish
scores$avg.finish <- 16 - round(scores$points / scores$maps, 1)
# order by score
scores <- scores[order(scores$points, decreasing = TRUE),]
rownames(scores) <- NULL
scores
# read in a maptimes condump
filename <- 'C:/quake2/jump/condumps/scores.txt'
# read in condump
scores <- scan(filename, what = 'raw', sep = '\n')
scores2 <- c()
i <- 1
for(x in scores) {
if(grepl('^\\d', x)) {
scores2[i] <- x
i <- i + 1
}
}
# write to a new text file for cleaner reading
write.table(scores2, 'cleaned.txt', row.names = FALSE, col.names = FALSE, quote = FALSE)
# read in new text file
scores <- read.table('cleaned.txt', header = FALSE)
scores <- scores[1:2]
#assign point values
points <- c(15:1)
scores$points <- points[scores$V1]
# aggregate the points for each person
scorescount <- aggregate(scores$points, by = list(scores$V2), FUN = length)
scores <- aggregate(scores$points, by = list(scores$V2), FUN = sum)
scores <- merge(x = scores, y = scorescount, by = "Group.1")
colnames(scores) = c('name','points','maps')
# calc avg finish
scores$avg.finish <- 16 - round(scores$points / scores$maps, 1)
# order by score
scores <- scores[order(scores$points, decreasing = TRUE),]
rownames(scores) <- NULL
scores
# read in a maptimes condump
filename <- 'C:/quake2/jump/condumps/scores.txt'
# read in condump
scores <- scan(filename, what = 'raw', sep = '\n')
scores2 <- c()
i <- 1
for(x in scores) {
if(grepl('^\\d', x)) {
scores2[i] <- x
i <- i + 1
}
}
# write to a new text file for cleaner reading
write.table(scores2, 'cleaned.txt', row.names = FALSE, col.names = FALSE, quote = FALSE)
# read in new text file
scores <- read.table('cleaned.txt', header = FALSE)
scores <- scores[1:2]
#assign point values
points <- c(15:1)
scores$points <- points[scores$V1]
# aggregate the points for each person
scorescount <- aggregate(scores$points, by = list(scores$V2), FUN = length)
scores <- aggregate(scores$points, by = list(scores$V2), FUN = sum)
scores <- merge(x = scores, y = scorescount, by = "Group.1")
colnames(scores) = c('name','points','maps')
# calc avg finish
scores$avg.finish <- 16 - round(scores$points / scores$maps, 1)
# order by score
scores <- scores[order(scores$points, decreasing = TRUE),]
rownames(scores) <- NULL
scores
# read in a maptimes condump
filename <- 'C:/quake2/jump/condumps/scores.txt'
# read in condump
scores <- scan(filename, what = 'raw', sep = '\n')
scores2 <- c()
i <- 1
for(x in scores) {
if(grepl('^\\d', x)) {
scores2[i] <- x
i <- i + 1
}
}
# write to a new text file for cleaner reading
write.table(scores2, 'cleaned.txt', row.names = FALSE, col.names = FALSE, quote = FALSE)
# read in new text file
scores <- read.table('cleaned.txt', header = FALSE)
scores <- scores[1:2]
#assign point values
points <- c(15:1)
scores$points <- points[scores$V1]
# aggregate the points for each person
scorescount <- aggregate(scores$points, by = list(scores$V2), FUN = length)
scores <- aggregate(scores$points, by = list(scores$V2), FUN = sum)
scores <- merge(x = scores, y = scorescount, by = "Group.1")
colnames(scores) = c('name','points','maps')
# calc avg finish
scores$avg.finish <- 16 - round(scores$points / scores$maps, 1)
# order by score
scores <- scores[order(scores$points, decreasing = TRUE),]
rownames(scores) <- NULL
scores
# read in a maptimes condump
filename <- 'C:/quake2/jump/condumps/scores.txt'
# read in condump
scores <- scan(filename, what = 'raw', sep = '\n')
scores2 <- c()
i <- 1
for(x in scores) {
if(grepl('^\\d', x)) {
scores2[i] <- x
i <- i + 1
}
}
# write to a new text file for cleaner reading
write.table(scores2, 'cleaned.txt', row.names = FALSE, col.names = FALSE, quote = FALSE)
# read in new text file
scores <- read.table('cleaned.txt', header = FALSE)
scores <- scores[1:2]
#assign point values
points <- c(15:1)
scores$points <- points[scores$V1]
# aggregate the points for each person
scorescount <- aggregate(scores$points, by = list(scores$V2), FUN = length)
scores <- aggregate(scores$points, by = list(scores$V2), FUN = sum)
scores <- merge(x = scores, y = scorescount, by = "Group.1")
colnames(scores) = c('name','points','maps')
# calc avg finish
scores$avg.finish <- 16 - round(scores$points / scores$maps, 1)
# order by score
scores <- scores[order(scores$points, decreasing = TRUE),]
rownames(scores) <- NULL
scores
# read in a maptimes condump
filename <- 'C:/quake2/jump/condumps/scores.txt'
# read in condump
scores <- scan(filename, what = 'raw', sep = '\n')
scores2 <- c()
i <- 1
for(x in scores) {
if(grepl('^\\d', x)) {
scores2[i] <- x
i <- i + 1
}
}
# write to a new text file for cleaner reading
write.table(scores2, 'cleaned.txt', row.names = FALSE, col.names = FALSE, quote = FALSE)
# read in new text file
scores <- read.table('cleaned.txt', header = FALSE)
scores <- scores[1:2]
#assign point values
points <- c(15:1)
scores$points <- points[scores$V1]
# aggregate the points for each person
scorescount <- aggregate(scores$points, by = list(scores$V2), FUN = length)
scores <- aggregate(scores$points, by = list(scores$V2), FUN = sum)
scores <- merge(x = scores, y = scorescount, by = "Group.1")
colnames(scores) = c('name','points','maps')
# calc avg finish
scores$avg.finish <- 16 - round(scores$points / scores$maps, 1)
# order by score
scores <- scores[order(scores$points, decreasing = TRUE),]
rownames(scores) <- NULL
scores
# read in a maptimes condump
filename <- 'C:/quake2/jump/condumps/scores.txt'
# read in condump
scores <- scan(filename, what = 'raw', sep = '\n')
scores2 <- c()
i <- 1
for(x in scores) {
if(grepl('^\\d', x)) {
scores2[i] <- x
i <- i + 1
}
}
# write to a new text file for cleaner reading
write.table(scores2, 'cleaned.txt', row.names = FALSE, col.names = FALSE, quote = FALSE)
# read in new text file
scores <- read.table('cleaned.txt', header = FALSE)
scores <- scores[1:2]
#assign point values
points <- c(15:1)
scores$points <- points[scores$V1]
# aggregate the points for each person
scorescount <- aggregate(scores$points, by = list(scores$V2), FUN = length)
scores <- aggregate(scores$points, by = list(scores$V2), FUN = sum)
scores <- merge(x = scores, y = scorescount, by = "Group.1")
colnames(scores) = c('name','points','maps')
# calc avg finish
scores$avg.finish <- 16 - round(scores$points / scores$maps, 1)
# order by score
scores <- scores[order(scores$points, decreasing = TRUE),]
rownames(scores) <- NULL
scores
scores[1,2]
scores[4,2]
scores[4,2] <- 80
scores
# read in a maptimes condump
filename <- 'C:/quake2/jump/condumps/scores.txt'
# read in condump
scores <- scan(filename, what = 'raw', sep = '\n')
scores2 <- c()
i <- 1
for(x in scores) {
if(grepl('^\\d', x)) {
scores2[i] <- x
i <- i + 1
}
}
# write to a new text file for cleaner reading
write.table(scores2, 'cleaned.txt', row.names = FALSE, col.names = FALSE, quote = FALSE)
# read in new text file
scores <- read.table('cleaned.txt', header = FALSE)
scores <- scores[1:2]
#assign point values
points <- c(15:1)
scores$points <- points[scores$V1]
# aggregate the points for each person
scorescount <- aggregate(scores$points, by = list(scores$V2), FUN = length)
scores <- aggregate(scores$points, by = list(scores$V2), FUN = sum)
scores <- merge(x = scores, y = scorescount, by = "Group.1")
colnames(scores) = c('name','points','maps')
# calc avg finish
scores$avg.finish <- 16 - round(scores$points / scores$maps, 1)
# order by score
scores <- scores[order(scores$points, decreasing = TRUE),]
rownames(scores) <- NULL
scores
data(pima, package="faraway")
install.packages("faraway")
data(pima, package="faraway")
data(pima, package="faraway")
head(pima)
require("plyr")
require("knitr")
require("psych")
training <- read.csv(url('https://raw.githubusercontent.com/rmalarc/DATA621/master/hw01/moneyball-training-data.csv'))
metadata <- read.csv(url('https://raw.githubusercontent.com/rmalarc/DATA621/master/hw01/moneyball-metadata.csv'))
kable(metadata)
columns <- colnames(training)
target <- "TARGET_WINS"
inputs <- columns[!columns %in% c(target,"INDEX")]
summary <- describe(training[,c(target,inputs)])[,c("n","mean","sd","median","max","min")]
summary
summary <- describe(training[,c(target,inputs)])[,c("n","mean","sd","median","max","min")]
summary$completeness <- summary$n/nrow(training)
kable(summary)
require("reshape2")
require("ggplot2")
ggplot(melt(training, measure.vars = c(target,inputs))
,aes(x=variable,y=value)
)+
geom_boxplot() +
coord_flip()
setwd("~/GitHub/DATA621/hw01")
opts_chunk$set(echo = TRUE)
par(mfrow=c(1,3))
ggplot(melt(training, measure.vars = c(target,inputs))
,aes(x=variable,y=value)
)+
geom_boxplot() +
coord_flip()
ggplot(melt(data.frame(scale(training)), measure.vars = c(target,inputs)),
aes(x=variable,y=value)
)+
geom_boxplot() +
coord_flip()
ggplot(melt(data.frame(scale(log(training))), measure.vars = c(target,inputs)),
aes(x=variable,y=value)
)+
geom_boxplot() +
coord_flip()
require("gridextra")
install.packages("gridextra")
install.packages("gridExtra")
require("gridExtra")
ggplot(melt(data.frame(scale(training)), measure.vars = c(target,inputs)),
aes(x=variable,y=value)
)+ geom_boxplot() + coord_flip() + ggtitle("Normalized Distribution")
g1 <- ggplot(melt(training, measure.vars = c(target,inputs))
,aes(x=variable,y=value)
)+ geom_boxplot() + coord_flip() + ggtitle("Raw Distribution")
g2 <- ggplot(melt(data.frame(scale(training)), measure.vars = c(target,inputs)),
aes(x=variable,y=value)
)+ geom_boxplot() + coord_flip() + ggtitle("Normalized Distribution")
g3 <- ggplot(melt(data.frame(scale(log(training))), measure.vars = c(target,inputs)),
aes(x=variable,y=value)
)+geom_boxplot() + coord_flip() + ggtitle("Log Distribution")
grid.arrange(g1, g2, g3, ncol = 1, main = "Variable Distribution")
grid.arrange(g1, g2, g3, ncol = 1, top = "Variable Distribution")
grid.arrange(g1, g2, g3, ncol = 3, top = "Variable Distribution")
summary$n <- NULL
summary
summary$n/nrow(training)
summary <- describe(training[,c(target,inputs)])[,c("n","mean","sd","median","max","min")]
summary$completeness <- summary$n/nrow(training)
summary$n <- NULL
kable(summary)
summary$n/nrow(training)
training <- read.csv(url('https://raw.githubusercontent.com/rmalarc/DATA621/master/hw01/moneyball-training-data.csv'))
metadata <- read.csv(url('https://raw.githubusercontent.com/rmalarc/DATA621/master/hw01/moneyball-metadata.csv'))
columns <- colnames(training)
target <- "TARGET_WINS"
inputs <- columns[!columns %in% c(target,"INDEX")]
summary <- describe(training[,c(target,inputs)])[,c("n","mean","sd","median","max","min")]
summary$completeness <- summary$n/nrow(training)
summary$n/nrow(training)
round(summary$n/nrow(training),2)
training <- read.csv(url('https://raw.githubusercontent.com/rmalarc/DATA621/master/hw01/moneyball-training-data.csv'))
metadata <- read.csv(url('https://raw.githubusercontent.com/rmalarc/DATA621/master/hw01/moneyball-metadata.csv'))
columns <- colnames(training)
target <- "TARGET_WINS"
inputs <- columns[!columns %in% c(target,"INDEX")]
summary <- describe(training[,c(target,inputs)])[,c("n","mean","sd","median","max","min")]
summary$completeness <- round(summary$n/nrow(training),2)
summary$n <- NULL
kable(summary)
cor
base_cor
base_cor <- cor(training[,c(target,inputs)], use="pairwise.complete.obs", method="kendall")
base_cor
base_cor[1,]
kable(base_cor[1,])
base_cor <- data.frame(base_cor[1,])
base_cor
base_cor <- data.frame(r = base_cor[1,])
kable()
kable(base_cor)
base_cor <- data.frame(r = base_cor[1,])
base_cor <- cor(training[,c(target,inputs)], use="pairwise.complete.obs", method="kendall")
base_cor <- data.frame(r = base_cor[1,])
kable(base_cor)
kable(t(base_cor))
qplot(base_cor)
require("MASS")
